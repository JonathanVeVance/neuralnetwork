{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from activfunc import relu, ..., activfunc_dict # relu is a class, activfunc is a file\n",
    "# from lossfunc import mse # mse is a class, lossfunc is a file\n",
    "# or just the dictionaries will do?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "activfunc_dict = {\n",
    "    'relu': relu, 'sigmoid': sigmoid, ..\n",
    "} # put in activfunc\n",
    "\n",
    "lossfunc_dict = {\n",
    "    'mse': mse, 'crossentropy': crossentropy, ...\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class neuralnetwork:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.W = []\n",
    "        self.Z = []\n",
    "        self.Y = []\n",
    "        self.Wgrad = None\n",
    "        self.nlayers = 0\n",
    "        self.activations = []\n",
    "        self.prev_layer_neurons = None\n",
    "        \n",
    "    def add_layer(self, num_neurons, activation = 'relu'):\n",
    "        ## TODO: add bias\n",
    "        if self.nlayers == 0:\n",
    "            self.W.append(None)\n",
    "        else:\n",
    "            weights = np.random.rand(num_neurons, self.prev_layer_neurons)\n",
    "            self.W.append(weights)\n",
    "        self.activations.append(activation.lower())\n",
    "        self.prev_layer_neurons = num_neurons\n",
    "        self.nlayers += 1\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        current_x = inputs\n",
    "        self.Y.append(inputs)\n",
    "        for k in range(nlayers):\n",
    "            z = (np.matmul(self.W[k], current_x.T)).T\n",
    "            Z.append(z)\n",
    "            \n",
    "            sigma = activfunc_dict[self.activations[k]]\n",
    "            y = sigma.forward(z)\n",
    "            self.Y.append(y)\n",
    "            current_x = y\n",
    "        return y\n",
    "        \n",
    "    def backward(self, op_gradient):\n",
    "        # Assuming op_gradient = row vector (convention)\n",
    "        # diag of Jacobian (scalar activation)\n",
    "        current_grad = op_gradient\n",
    "        for k in range(nlayers, -1, -1): # opvec = Y[nlayers]\n",
    "            sigma = activfunc_dict[self.activations[k - 1]]\n",
    "            gradDz = np.multiply(current_grad, sigma.backward(self.Z[k - 1])) \n",
    "            # if local grad = matrix (Jacobian), use matmul else multiply\n",
    "            gradzw = np.matmul(np.ones(np.shape(W[k - 1])[0]), self.Y[k - 1])\n",
    "            self.Wgrad[k - 1] = np.matmul(gradDz, gradzw)\n",
    "            current_grad = np.matmul(gradDz, self.W[k - 1])\n",
    "        \n",
    "    def train_network(self, X_train, Y_train, method = 'SGD', loss_function, eta = 0.001):\n",
    "        \n",
    "        input_size = np.shape(X_train)[1]\n",
    "        layer1_size = np.shape(self.W[1])[1]\n",
    "        self.W[0] = np.random.rand(layer1_size, input_size)\n",
    "        self.Wgrad = np.zeros(np.shape(self.W))\n",
    "        lossfunc = lossfunc_dict[loss_function.lower()]\n",
    "        \n",
    "        prev_loss = 0\n",
    "        while True:\n",
    "            permut = np.random.permutation(len(Y_train))\n",
    "            X_train = X_train[permut]\n",
    "            Y_train = Y_train\n",
    "            for idx, x in enumerate(X_train):\n",
    "                \n",
    "                Y_pred = self.forward(x)\n",
    "                loss = lossfunc(Y_pred, Y_train[idx])\n",
    "                \n",
    "                if abs(prev_loss - loss) < 0.01: \n",
    "                    break # stopping condition \n",
    "                    \n",
    "                op_gradient = lossfunc.gradient(Y_pred, Y_train[idx])\n",
    "                self.backward(op_gradient)\n",
    "                self.W -= eta * self.Wgrad # SGD update\n",
    "                prev_loss = loss\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = network(8)\n",
    "NN.add_layer(12, activation = 'relu')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
