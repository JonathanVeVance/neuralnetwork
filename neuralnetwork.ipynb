{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from helpers.activfunc import ReLu, sigmoid\n",
    "from helpers.lossfunc import sum_squares, cross_entropy\n",
    "from helpers.regularizer import l2_regularizer, l1_regularizer # make an instance when calling train_model\n",
    "from helpers.grad_descent import SGD, MiniBatchGD\n",
    "from helpers.accelerator import adam, rmsprop, momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "activfunc_dict = {\n",
    "    'relu': ReLu, 'sigmoid': sigmoid\n",
    "}\n",
    "\n",
    "lossfunc_dict = {\n",
    "    'sumsquares': sum_squares, 'crossentropy': cross_entropy\n",
    "}\n",
    "\n",
    "grad_descent_dict = {\n",
    "    'sgd': SGD, 'minibatchgd': MiniBatchGD\n",
    "}\n",
    "\n",
    "accelerator_dict = {\n",
    "    'adam': adam, 'rmsprop': rmsprop, 'momentum': momentum\n",
    "}\n",
    "\n",
    "regularizer_dict = {\n",
    "    'l2': l2_regularizer, 'l1': l1_regularizer\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class neuralnetwork:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.W = []\n",
    "        self.Z = []\n",
    "        self.Y = []\n",
    "        self.Wgrad = None\n",
    "        self.nlayers = 0\n",
    "        self.dropouts = []\n",
    "        self.activations = []\n",
    "        self.prev_layer_neurons = None\n",
    "        \n",
    "    def add_layer(self, num_neurons, activation, dropout = None):\n",
    "        if self.nlayers == 0:\n",
    "            self.W.append(None)\n",
    "        else:\n",
    "            weights = np.random.rand(num_neurons, self.prev_layer_neurons + 1)\n",
    "            self.W.append(weights)\n",
    "        \n",
    "        self.dropouts.append(dropout)\n",
    "        self.activations.append(activation.lower())\n",
    "        self.prev_layer_neurons = num_neurons\n",
    "        self.nlayers += 1\n",
    "    \n",
    "    def forward(self, inputs, predict = False):\n",
    "        \n",
    "        current_x = inputs\n",
    "        np.insert(current_x, 0, 1, axis = 0)\n",
    "        self.Y.append(current_x)\n",
    "        for k in range(nlayers):\n",
    "            z = (np.matmul(self.W[k], current_x.T)).T\n",
    "            Z.append(z)\n",
    "            \n",
    "            sigma = activfunc_dict[self.activations[k]]\n",
    "            y = sigma.forward(z)\n",
    "            if predict == False and self.dropouts[k] != None:\n",
    "                 y /= self.dropouts[k]\n",
    "            self.Y.append(y)\n",
    "            current_x = y\n",
    "            np.insert(current_x, 0, 1, axis = 0)\n",
    "                        \n",
    "        return y\n",
    "        \n",
    "    def backward(self, op_gradient, regularizer):\n",
    "        current_grad = op_gradient\n",
    "        for k in range(nlayers, -1, -1): \n",
    "            sigma = activfunc_dict[self.activations[k - 1]]\n",
    "            gradDz = np.matmul(current_grad, sigma.backward(self.Z[k - 1]))\n",
    "            gradzw = np.matmul(np.ones(np.shape(W[k - 1])[0]), self.Y[k - 1])\n",
    "            self.Wgrad[k - 1] = np.matmul(gradDz, gradzw)\n",
    "            if regularizer != None:\n",
    "                self.Wgrad[k - 1] += regularizer.gradient(self.W[k - 1])\n",
    "            current_grad = np.matmul(gradDz, self.W[k - 1]) \n",
    "    \n",
    "    def clear_outputs(self):\n",
    "        self.Z = []\n",
    "        self.Y = []\n",
    "        \n",
    "    def train_network(self, X_train, Y_train, loss_function, grad_descent_type = 'minibatchgd',  \n",
    "                      learning_rate = 0.001, regularizer = None, accelerator = None):\n",
    "        \n",
    "        input_size = np.shape(X_train)[1]\n",
    "        layer1_size = np.shape(self.W[1])[1]\n",
    "        self.W[0] = np.random.rand(layer1_size, input_size + 1)\n",
    "        self.Wgrad = np.zeros(np.shape(self.W))\n",
    "        loss_function = lossfunc_dict[loss_function.lower()]\n",
    "        regularizer = regularizer_dict[regularizer.lower()]\n",
    "        accelerator = accelerator_dict[accelerator.lower()]\n",
    "        grad_descent = grad_descent_dict[grad_descent_type.lower()]\n",
    "        grad_descent(self, X_train, Y_train, loss_function, learning_rate, regularizer, accelerator)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \n",
    "        Y_test_pred = []\n",
    "        for x in X_test:\n",
    "            y = self.forward(x, predict = True)\n",
    "            Y_test_pred.append(y)\n",
    "            \n",
    "        return Y_test_pred\n",
    "       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
